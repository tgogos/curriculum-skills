# ğŸ§ª SkillCrawl Unit Test Suite

This test suite ensures the quality of SkillCrawlâ€™s outputs across multiple dimensions such as correctness, integrity, and completeness.

Tests are categorized into the following quality dimensions:

| Category              | Goal                                                                 |
|-----------------------|----------------------------------------------------------------------|
| âœ… Data Integrity      | No duplicates or corrupted entries                                   |
| ğŸ“ Consistency         | Stable, explainable patterns in results                             |
| ğŸ“Š Completeness        | No missing or under-analyzed lessons                                |
| ğŸ¯ Accuracy            | Extracted skills match expected outcomes from trusted datasets       |

---

## ğŸ“ Structure

```
tests/
â”œâ”€â”€ json/
â”‚   â””â”€â”€ [Expected output JSON files]
â”œâ”€â”€ sample_pdfs/
â”‚   â””â”€â”€ test_curriculum.pdf
â”œâ”€â”€ test_compare_extracted_titles.py
â”œâ”€â”€ test_compare_extracted_skills.py
â”œâ”€â”€ test_compare_top_skills.py
â”œâ”€â”€ test_compare_skill_search.py
â””â”€â”€ ...
```

---

## âœ… Existing Test Cases

### ğŸ¯ Accuracy Testing

#### 1. Compare Extracted Titles from PDF  
**File:** `test_compare_extracted_titles.py`   
**Compares:** Method output â†’ `extracted_titles_expected.json`  
**Difficulty:** ğŸŸ© Easy  

**Purpose:** Extracts titles from a sample PDF using `process_pdf()` and checks them against the confirmed set.

- **Method Called:** `process_pdf(PDFProcessingRequest)`
- **Expected File:** `json/extracted_titles_expected.json`

ğŸ” Compares returned lesson titles (per semester) directly to the expected JSON.
âœ”ï¸ Validates that all course titles are correctly extracted per semester.


---

#### 2. Skill Extraction from Course Descriptions  
**File:** `test_compare_extracted_skills.py`  
**Compares:** Method output â†’ `extracted_skills_expected.json`  
**Difficulty:** ğŸŸ¨ Medium  
**Purpose:** Uses `process_pdf()` and `calculate_skillnames()` to extract skills from course descriptions and compare to expected.

- **Methods Called:**  
  - `process_pdf(PDFProcessingRequest)`  
  - `calculate_skillnames(university_name)`

- **Expected File:** `json/extracted_skills_expected.json`

ğŸ§  Confirms that all expected skills per lesson are detected, including names.
âœ”ï¸ Ensures that skills extracted from lesson descriptions match confirmed ones.


---

#### 3. Top N Skills for a University  
**File:** `test_compare_top_skills.py`  
**Compares:** Method output â†’ `top_skills_cambridge_expected.json`  
**Difficulty:** ğŸŸ© Easy  
**Purpose:** Verifies that `get_top_skills()` returns the correct top N skills for a university with the expected frequency.

- **Method Called:** `get_top_skills(TopSkillsRequest)`
- **Expected File:** `json/top_skills_cambridge_expected.json`

ğŸ“Š Checks that skill names and counts match expected results.
âœ”ï¸ Checks top ESCO skills returned by the system and their frequency.

---

#### 4. Skill-based Search Results  
**File:** `test_compare_skill_search.py`  
**Method:** `search_skill()`  
**Difficulty:** ğŸŸ¨ Medium  
**Purpose:** Runs a live skill-based course search and compares the results to verified course/skill/university matches.

- **Method Called:** `search_skill(SkillSearchRequest)`
- **Expected File:** `json/C_skill_search_expected.json`

ğŸ” Ensures that returned results match the expected structure and content, including frequency and fuzzy score.
âœ”ï¸ Verifies courses matched to a given skill (across all universities).

---

## ğŸ§ª Proposed New Tests (for expansion)

These are **not yet implemented** purposefully â€” they are ideal for a challenge at the testathon!

---

### âœ… Data Integrity Testing

#### 5. Duplicate Skills Per Lesson  
**Goal:** Ensure no skill is assigned more than once to the same lesson.  
**Method(s):** Check `calculate_skillnames()` output  
**Difficulty:** ğŸŸ© Easy  
âœ”ï¸ Loop through all lessons and ensure all skill names are unique.

---

#### 6. Duplicate Skills Across University  
**Goal:** Detect redundant skill entries across multiple lessons in the same university.  
**Method(s):** Use `get_skills_for_lesson()`  
**Difficulty:** ğŸŸ¨ Medium  
âœ”ï¸ Count how often each skill appears and detect unnecessary repetition.

---

### ğŸ“ Consistency Testing

#### 7. Cross-Course Skill Similarity  
**Goal:** Ensure that related lessons (e.g., under the same department or semester) share logically consistent skill sets.  
**Method(s):** Compare lesson skill sets using similarity scoring.  
**Difficulty:** ğŸŸ¥ Hard  
âœ”ï¸ Flag anomalies where unrelated skill sets appear within similar contexts.

---

#### 8. Inconsistent Skill Distribution  
**Goal:** Detect universities where similar lesson names have completely disjoint skills.  
**Method(s):** Cluster lessons and analyze skill overlap.  
**Difficulty:** ğŸŸ¥ Hard  
âœ”ï¸ Suggest possible errors or inconsistencies in extraction.

---

### ğŸ“Š Completeness Testing

#### 9. Missing Skills in Lessons  
**Goal:** Flag any lessons with zero skills after processing.  
**Method(s):** `calculate_skillnames()`  
**Difficulty:** ğŸŸ© Easy  
âœ”ï¸ Useful to catch bad input data or failed OCR.

---

#### 10. Minimum Skill Threshold  
**Goal:** Validate that each lesson has at least N skills (e.g., 3).  
**Method(s):** Compare count from skill output.  
**Difficulty:** ğŸŸ¨ Medium  
âœ”ï¸ Helps surface under-analyzed lessons.

---

### ğŸ¯ Accuracy Testing (Advanced)

#### 11. Skill Mapping Validation with ESCO Labels  
**Goal:** Compare actual ESCO preferred labels with what was extracted.  
**Method(s):** Use `extract_and_get_title()`  
**Difficulty:** ğŸŸ¨ Medium  
âœ”ï¸ Detect mismatches between ESCO URLs and their resolved names.

---

#### 12. API JSON Schema Validation  
**Goal:** Ensure the API responses match expected structure and required fields.  
**Method(s):** FastAPI schema + `pydantic`  
**Difficulty:** ğŸŸ¨ Medium  
âœ”ï¸ Useful for future-proofing API integrations.

---

## â–¶ï¸ How to Run Tests

Install requirements:

```bash
pip install -r requirements.txt
```

Run all tests:

```bash
python pytest -m tests/
```
> Hint: Running this will return you a percentage of successful match.

Run a specific test:

```bash
python pytest -m tests/test_compare_extracted_skills.py
```
> Tip: Make sure you are in the curriculum-skills folder!
> Hint: Running this will return you a percentage of successful match.

Run tests, but informative ones, to see exactly what the test returned:

```bash
python -m pytest -s tests/test_compare_extracted_skills.py
```
>Tip: You add the -s to make sure you also get information about what was checked!
>It is solely for testing purposes. It is more informative than just a percentage!

---

## ğŸ§  Summary Table

| Test Case                            | Type               | Difficulty |
|-------------------------------------|--------------------|------------|
| Title extraction                    | ğŸ¯ Accuracy         | ğŸŸ© Easy     |
| Skill extraction from descriptions  | ğŸ¯ Accuracy         | ğŸŸ¨ Medium   |
| Top skills per university           | ğŸ¯ Accuracy         | ğŸŸ© Easy     |
| Skill-based search                  | ğŸ¯ Accuracy         | ğŸŸ¨ Medium   |
| Detect duplicate skills             | âœ… Data Integrity    | ğŸŸ© Easy     |
| Duplicate across university         | âœ… Data Integrity    | ğŸŸ¨ Medium   |
| Cross-course similarity             | ğŸ“ Consistency       | ğŸŸ¥ Hard     |
| Inconsistent skill distribution     | ğŸ“ Consistency       | ğŸŸ¥ Hard     |
| Missing skills                      | ğŸ“Š Completeness      | ğŸŸ© Easy     |
| Minimum skills per lesson           | ğŸ“Š Completeness      | ğŸŸ¨ Medium   |
| ESCO label mismatch                 | ğŸ¯ Accuracy         | ğŸŸ¨ Medium   |
| API schema check                    | ğŸ¯ Accuracy         | ğŸŸ¨ Medium   |
